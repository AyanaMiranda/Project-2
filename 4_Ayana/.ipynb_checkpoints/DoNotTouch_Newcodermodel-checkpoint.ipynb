{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "964c9180-6e36-4e2b-bf3e-46f2ee4dd86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_9960\\2400943436.py:11: DtypeWarning: Columns (17,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_cleaned = pd.read_csv('final_processed_data-Copy1.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>AttendedBootcamp</th>\n",
       "      <th>BootcampFinish</th>\n",
       "      <th>BootcampLoanYesNo</th>\n",
       "      <th>BootcampName</th>\n",
       "      <th>BootcampRecommend</th>\n",
       "      <th>ChildrenNumber</th>\n",
       "      <th>CityPopulation</th>\n",
       "      <th>CodeEventConferences</th>\n",
       "      <th>CodeEventDjangoGirls</th>\n",
       "      <th>...</th>\n",
       "      <th>YouTubeFCC</th>\n",
       "      <th>YouTubeFunFunFunction</th>\n",
       "      <th>YouTubeGoogleDev</th>\n",
       "      <th>YouTubeLearnCode</th>\n",
       "      <th>YouTubeLevelUpTuts</th>\n",
       "      <th>YouTubeMIT</th>\n",
       "      <th>YouTubeMozillaHacks</th>\n",
       "      <th>YouTubeOther</th>\n",
       "      <th>YouTubeSimplilearn</th>\n",
       "      <th>YouTubeTheNewBoston</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>more than 1 million</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>less than 100,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>more than 1 million</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>between 100,000 and 1 million</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>between 100,000 and 1 million</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  AttendedBootcamp  BootcampFinish  BootcampLoanYesNo BootcampName  \\\n",
       "0  0.300000               0.0             NaN                NaN          NaN   \n",
       "1  0.377778               0.0             NaN                NaN          NaN   \n",
       "2  0.233333               0.0             NaN                NaN          NaN   \n",
       "3  0.288889               0.0             NaN                NaN          NaN   \n",
       "4  0.222222               0.0             NaN                NaN          NaN   \n",
       "\n",
       "   BootcampRecommend  ChildrenNumber                 CityPopulation  \\\n",
       "0                NaN             NaN            more than 1 million   \n",
       "1                NaN             NaN              less than 100,000   \n",
       "2                NaN             NaN            more than 1 million   \n",
       "3                NaN             NaN  between 100,000 and 1 million   \n",
       "4                NaN             NaN  between 100,000 and 1 million   \n",
       "\n",
       "   CodeEventConferences  CodeEventDjangoGirls  ...  YouTubeFCC  \\\n",
       "0                   NaN                   NaN  ...         NaN   \n",
       "1                   NaN                   NaN  ...         0.0   \n",
       "2                   NaN                   NaN  ...         NaN   \n",
       "3                   NaN                   NaN  ...         0.0   \n",
       "4                   NaN                   NaN  ...         NaN   \n",
       "\n",
       "   YouTubeFunFunFunction  YouTubeGoogleDev  YouTubeLearnCode  \\\n",
       "0                    NaN               NaN               NaN   \n",
       "1                    NaN               NaN               NaN   \n",
       "2                    NaN               NaN               0.0   \n",
       "3                    0.0               NaN               NaN   \n",
       "4                    NaN               NaN               NaN   \n",
       "\n",
       "   YouTubeLevelUpTuts  YouTubeMIT  YouTubeMozillaHacks YouTubeOther  \\\n",
       "0                 NaN         NaN                  NaN          NaN   \n",
       "1                 NaN         NaN                  NaN          NaN   \n",
       "2                 0.0         NaN                  NaN          NaN   \n",
       "3                 0.0         NaN                  NaN          NaN   \n",
       "4                 NaN         NaN                  NaN          NaN   \n",
       "\n",
       "   YouTubeSimplilearn  YouTubeTheNewBoston  \n",
       "0                 NaN                  NaN  \n",
       "1                 NaN                  NaN  \n",
       "2                 NaN                  NaN  \n",
       "3                 NaN                  NaN  \n",
       "4                 NaN                  NaN  \n",
       "\n",
       "[5 rows x 136 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "data_cleaned = pd.read_csv('final_processed_data-Copy1.csv')\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b81d378d-d32b-489f-8c58-9bb2d0277bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18175 entries, 0 to 18174\n",
      "Columns: 136 entries, Age to YouTubeTheNewBoston\n",
      "dtypes: float64(105), object(31)\n",
      "memory usage: 18.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data_cleaned.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a92d086-483f-4007-a36c-8341423c77f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                    float64\n",
       "AttendedBootcamp       float64\n",
       "BootcampFinish         float64\n",
       "BootcampLoanYesNo      float64\n",
       "BootcampName            object\n",
       "                        ...   \n",
       "YouTubeMIT             float64\n",
       "YouTubeMozillaHacks    float64\n",
       "YouTubeOther            object\n",
       "YouTubeSimplilearn     float64\n",
       "YouTubeTheNewBoston    float64\n",
       "Length: 136, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d5cd919-c01c-44bf-92af-2af468aea6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                     2808\n",
      "AttendedBootcamp         466\n",
      "BootcampFinish         17106\n",
      "BootcampLoanYesNo      17096\n",
      "BootcampName           17226\n",
      "                       ...  \n",
      "YouTubeMIT             14848\n",
      "YouTubeMozillaHacks    17553\n",
      "YouTubeOther           17027\n",
      "YouTubeSimplilearn     17974\n",
      "YouTubeTheNewBoston    15215\n",
      "Length: 136, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data_cleaned.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a950b098-eea6-49ed-9858-78fa4a174801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Handle missing values by removing rows with missing values\n",
    "data_cleaned = data_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9c12f94-d959-403d-97fe-baf52052d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Remove duplicate records\n",
    "data_cleaned = data_cleaned.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f45a6054-746b-49bc-ad8a-bfdc15fd7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Merge Columns\n",
    "# Merge 'Do you financially support any dependents?' and 'Do you have children?' into 'Dependents'\n",
    "def merge_dependents(row):\n",
    "    # Check if both columns exist and apply logic\n",
    "    if 'Do you financially support any dependents?' in row.index and 'Do you have children?' in row.index:\n",
    "        if row['Do you financially support any dependents?'] == 'Yes' or row['Do you have children?'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No'\n",
    "    return np.nan  # Return NaN if columns do not exist\n",
    "# Apply the function to create the 'Dependents' column\n",
    "data_cleaned['Dependents'] = data_cleaned.apply(merge_dependents, axis=1)\n",
    "# Merge 'Do you have student loan debt?', 'Do you have any debt?', and 'Do you have a home mortgage?' into 'Debt status category'\n",
    "def merge_debt(row):\n",
    "    if 'Do you have student loan debt?' in row.index and row['Do you have student loan debt?'] == 'Yes':\n",
    "        return 'Student Loan'\n",
    "    elif 'Do you have a home mortgage?' in row.index and row['Do you have a home mortgage?'] == 'Yes':\n",
    "        return 'Mortgage'\n",
    "    elif 'Do you have any debt?' in row.index and row['Do you have any debt?'] == 'Yes':\n",
    "        return 'Other Debt'\n",
    "    else:\n",
    "        return 'No Debt'\n",
    "# Apply the function to create the 'Debt status category' column\n",
    "data_cleaned['Debt status category'] = data_cleaned.apply(merge_debt, axis=1)\n",
    "# Merge 'Other' columns ('Other' gender, employment/school, career)\n",
    "def merge_other(row):\n",
    "    if 'Other' in row.index and pd.notna(row['Other']):\n",
    "        return row['Other']\n",
    "    elif 'Other.1' in row.index and pd.notna(row['Other.1']):\n",
    "        return row['Other.1']\n",
    "    elif 'Other.2' in row.index and pd.notna(row['Other.2']):\n",
    "        return row['Other.2']\n",
    "    return np.nan\n",
    "# Apply the function to merge 'Other' columns\n",
    "data_cleaned['Other'] = data_cleaned.apply(merge_other, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fe6e90c-6311-4d3e-aa66-1ff67977e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Remove irrelevant columns\n",
    "columns_to_remove = ['Submit Date (UTC)', 'Start Date (UTC)', 'Network ID', 'Other.1', 'Other.2', 'Other']\n",
    "data_cleaned = data_cleaned.drop(columns=[col for col in columns_to_remove if col in data_cleaned.columns])\n",
    "# Step 5: Handle any inconsistencies in the data (assuming none for now)\n",
    "# The data is now cleaned and ready for machine learning algorithms.\n",
    "# Optional: Save the cleaned data to a new CSV file\n",
    "data_cleaned.to_csv('cleaned_survey_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9b7cd83-0bc0-464b-94f7-aca9bb269d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Remove irrelevant columns if they exist in the dataset\n",
    "columns_to_remove = ['Submit Date (UTC)', 'Start Date (UTC)', 'Network ID', '#', 'Other', 'Other.1', 'Other.2']\n",
    "# Check if each column exists before dropping\n",
    "data_cleaned = data_cleaned.drop(columns=[col for col in columns_to_remove if col in data_cleaned.columns], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1c6fbaa-536c-44ee-b195-d509c1457c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Age, AttendedBootcamp, BootcampFinish, BootcampLoanYesNo, BootcampName, BootcampRecommend, ChildrenNumber, CityPopulation, CodeEventConferences, CodeEventDjangoGirls, CodeEventFCC, CodeEventGameJam, CodeEventGirlDev, CodeEventHackathons, CodeEventMeetup, CodeEventNodeSchool, CodeEventNone, CodeEventOther, CodeEventRailsBridge, CodeEventRailsGirls, CodeEventStartUpWknd, CodeEventWkdBootcamps, CodeEventWomenCode, CodeEventWorkshops, CommuteTime, CountryCitizen, CountryLive, EmploymentField, EmploymentFieldOther, EmploymentStatus, EmploymentStatusOther, ExpectedEarning, FinanciallySupporting, FirstDevJob, Gender, GenderOther, HasChildren, HasDebt, HasFinancialDependents, HasHighSpdInternet, HasHomeMortgage, HasServedInMilitary, HasStudentDebt, HomeMortgageOwe, HoursLearning, ID.x, ID.y, Income, IsEthnicMinority, IsReceiveDisabilitiesBenefits, IsSoftwareDev, IsUnderEmployed, JobApplyWhen, JobInterestBackEnd, JobInterestDataEngr, JobInterestDataSci, JobInterestDevOps, JobInterestFrontEnd, JobInterestFullStack, JobInterestGameDev, JobInterestInfoSec, JobInterestMobile, JobInterestOther, JobInterestProjMngr, JobInterestQAEngr, JobInterestUX, JobPref, JobRelocateYesNo, JobRoleInterest, JobWherePref, LanguageAtHome, MaritalStatus, MoneyForLearning, MonthsProgramming, NetworkID, Part1EndTime, Part1StartTime, Part2EndTime, Part2StartTime, PodcastChangeLog, PodcastCodeNewbie, PodcastCodePen, PodcastDevTea, PodcastDotNET, PodcastGiantRobots, PodcastJSAir, PodcastJSJabber, PodcastNone, PodcastOther, PodcastProgThrowdown, PodcastRubyRogues, PodcastSEDaily, PodcastSERadio, PodcastShopTalk, PodcastTalkPython, PodcastTheWebAhead, ResourceCodecademy, ResourceCodeWars, ResourceCoursera, ResourceCSS, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 138 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Display cleaned data (or save it to a new CSV file)\n",
    "print(data_cleaned.head())  # Display the first few rows\n",
    "# If you want to save the cleaned data to a new file\n",
    "data_cleaned.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "919dad59-bfc9-46b8-907a-58b91aac805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Label Encoding for small categories and ordinal features\n",
    "label_encoder = LabelEncoder()\n",
    "# Label encode Gender (if binary or small categories)\n",
    "if 'Gender' in data_cleaned.columns:\n",
    "    data_cleaned['Gender_Encoded'] = label_encoder.fit_transform(data_cleaned['Gender'])\n",
    "# Label encode Degree Level (ordinal)\n",
    "if 'Degree Level' in data_cleaned.columns:\n",
    "    data_cleaned['Degree_Level_Encoded'] = label_encoder.fit_transform(data_cleaned['Degree Level'])\n",
    "# Label encode Employment Status\n",
    "if 'Employment Status' in data_cleaned.columns:\n",
    "    data_cleaned['Employment_Status_Encoded'] = label_encoder.fit_transform(data_cleaned['Employment Status'])\n",
    "# Label encode Student Loan Debt (Yes/No or binary)\n",
    "if 'Do you have student loan debt?' in data_cleaned.columns:\n",
    "    data_cleaned['Student_Loan_Debt_Encoded'] = label_encoder.fit_transform(data_cleaned['Do you have student loan debt?'])\n",
    "# Label encode Family Responsibilities (Yes/No for dependents, children)\n",
    "if 'Dependents' in data_cleaned.columns:\n",
    "    data_cleaned['Dependents_Encoded'] = label_encoder.fit_transform(data_cleaned['Dependents'])\n",
    "# Step 2: One-Hot Encoding for unordered categorical variables\n",
    "# One-hot encode Country of Residence\n",
    "if 'Country of Residence' in data_cleaned.columns:\n",
    "    data_cleaned = pd.get_dummies(data_cleaned, columns=['Country of Residence'], prefix='Country')\n",
    "# One-hot encode Citizenship\n",
    "if 'Citizenship' in data_cleaned.columns:\n",
    "    data_cleaned = pd.get_dummies(data_cleaned, columns=['Citizenship'], prefix='Citizenship')\n",
    "# One-hot encode Job Roles\n",
    "if 'Job Roles' in data_cleaned.columns:\n",
    "    data_cleaned = pd.get_dummies(data_cleaned, columns=['Job Roles'], prefix='Job_Role')\n",
    "# One-hot encode Field of Study\n",
    "if 'Field of Study' in data_cleaned.columns:\n",
    "    data_cleaned = pd.get_dummies(data_cleaned, columns=['Field of Study'], prefix='Field_Study')\n",
    "# One-hot encode Learning Preferences\n",
    "if 'Learning Preferences' in data_cleaned.columns:\n",
    "    data_cleaned = pd.get_dummies(data_cleaned, columns=['Learning Preferences'], prefix='Learning_Pref')\n",
    "# One-hot encode Coding Events/Workshops\n",
    "if 'Coding Events/Workshops' in data_cleaned.columns:\n",
    "    data_cleaned = pd.get_dummies(data_cleaned, columns=['Coding Events/Workshops'], prefix='Coding_Event')\n",
    "# One-hot encode Employment Type Preferences\n",
    "if 'Employment Type Preferences' in data_cleaned.columns:\n",
    "    data_cleaned = pd.get_dummies(data_cleaned, columns=['Employment Type Preferences'], prefix='Employment_Type')\n",
    "# Optional: Step 3: Handling High-Cardinality Features\n",
    "# For very high-cardinality categorical features like 'Country', if dimensionality becomes an issue,\n",
    "# consider using binary encoding from the 'category_encoders' library.\n",
    "# Example: Using binary encoding for 'Country' feature with many categories\n",
    "# Install the category_encoders package if needed:\n",
    "# !pip install category_encoders\n",
    "# from category_encoders import BinaryEncoder\n",
    "# encoder = BinaryEncoder(cols=['Country'])\n",
    "# data_cleaned = encoder.fit_transform(data_cleaned)\n",
    "# Save the encoded dataset to a new CSV file\n",
    "data_cleaned.to_csv('encoded_survey_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74c7a2ca-ffe3-4ad4-96e5-ee8e455ea510",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './clean-data/2017-fCC-New-Coders-Survey-Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load the dataset (replace with your actual file path)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data_cleaned \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./clean-data/2017-fCC-New-Coders-Survey-Data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Identify Numerical Columns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Select only numerical columns (int64 and float64)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m numerical_columns \u001b[38;5;241m=\u001b[39m data_cleaned\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './clean-data/2017-fCC-New-Coders-Survey-Data.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Load the dataset (replace with your actual file path)\n",
    "data_cleaned = pd.read_csv('./clean-data/2017-fCC-New-Coders-Survey-Data.csv')\n",
    "# Step 1: Identify Numerical Columns\n",
    "# Select only numerical columns (int64 and float64)\n",
    "numerical_columns = data_cleaned.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# Step 2: Exclude binary or categorical columns from scaling\n",
    "# Define the binary or categorical columns that should not be scaled\n",
    "exclude_columns = ['Dependents', 'Gender', 'Employment Status']  # Add any other binary or categorical columns here\n",
    "# Identify the numerical columns to scale by excluding the binary/categorical columns\n",
    "columns_to_scale = [col for col in numerical_columns if col not in exclude_columns]\n",
    "print(\"Columns to scale: \", columns_to_scale)\n",
    "# Step 3: Normalize the selected columns using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_cleaned[columns_to_scale] = scaler.fit_transform(data_cleaned[columns_to_scale])\n",
    "# Step 4: Standardize the selected columns using StandardScaler (if you want to standardize instead of normalize)\n",
    "# scaler = StandardScaler()\n",
    "# data_cleaned[columns_to_scale] = scaler.fit_transform(data_cleaned[columns_to_scale])\n",
    "# The data is now normalized (or standardized if you uncomment the StandardScaler)\n",
    "# Optional: Save the final processed data\n",
    "data_cleaned.to_csv('final_processed_data.csv', index=False)\n",
    "# Check the first few rows of the scaled data\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9400d70c-9c85-40dc-b8e7-c4e73d78fd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in 'AttendedBootcamp' and 'BootcampFinish'\n",
    "print(data_cleaned['AttendedBootcamp'].isnull().sum())\n",
    "print(data_cleaned['BootcampFinish'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4016fe0c-184c-461a-891d-d45c882f6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'AttendedBootcamp' or 'BootcampFinish' have NaN values\n",
    "data_cleaned = data_cleaned.dropna(subset=['AttendedBootcamp', 'BootcampFinish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c63ea0b2-6392-439e-aafe-55c863c230d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and targets ('AttendedBootcamp' and 'BootcampFinish') again after removing NaNs\n",
    "X = data_cleaned.drop(columns=['AttendedBootcamp', 'BootcampFinish'])  # Features\n",
    "y_attend = data_cleaned['AttendedBootcamp']  # Target 1: Attend Bootcamp\n",
    "y_finish = data_cleaned['BootcampFinish']  # Target 2: Finish Bootcamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d477b819-0fff-4e96-800f-fddadee0714a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Split data for 'AttendedBootcamp' prediction with stratified sampling\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_train_attend, X_test_attend, y_train_attend, y_test_attend \u001b[38;5;241m=\u001b[39m train_test_split(X, y_attend, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_attend, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data for 'AttendedBootcamp' prediction with stratified sampling\n",
    "X_train_attend, X_test_attend, y_train_attend, y_test_attend = train_test_split(X, y_attend, test_size=0.3, stratify=y_attend, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7fbf5f0-3841-4522-ad07-6e97e284ccb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split data for 'BootcampFinish' prediction with stratified sampling\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train_finish, X_test_finish, y_train_finish, y_test_finish \u001b[38;5;241m=\u001b[39m train_test_split(X, y_finish, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_finish, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Split data for 'BootcampFinish' prediction with stratified sampling\n",
    "X_train_finish, X_test_finish, y_train_finish, y_test_finish = train_test_split(X, y_finish, test_size=0.3, stratify=y_finish, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8349c75-0429-4a04-bb14-5764eb197c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attend = pd.concat([X_train_attend, y_train_attend], axis=1)\n",
    "test_attend = pd.concat([X_test_attend, y_test_attend], axis=1)\n",
    "train_attend['Set'] = 'Train'\n",
    "test_attend['Set'] = 'Test'\n",
    "combined_attend = pd.concat([train_attend, test_attend])\n",
    "combined_attend.to_csv('attended_bootcamp_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37677c6-c591-4472-9484-d7d18ecc9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'BootcampFinish' target\n",
    "train_finish = pd.concat([X_train_finish, y_train_finish], axis=1)\n",
    "test_finish = pd.concat([X_test_finish, y_test_finish], axis=1)\n",
    "train_finish['Set'] = 'Train'\n",
    "test_finish['Set'] = 'Test'\n",
    "combined_finish = pd.concat([train_finish, test_finish])\n",
    "combined_finish.to_csv('bootcamp_finish_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0d7eb-0308-4345-b939-35d11afb553f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0cbcf-d7a5-4ed1-ba98-0ba5ac1f430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for each member (you can modify these paths as needed)\n",
    "path_ayana = \"datasets/ayana/\"\n",
    "path_roberta = \"datasets/roberta/\"\n",
    "path_dom = \"datasets/dom/\"\n",
    "path_phillip = \"datasets/phillip/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968f8b9-7d2b-4209-93b3-466be824f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(path_ayana, exist_ok=True)\n",
    "os.makedirs(path_roberta, exist_ok=True)\n",
    "os.makedirs(path_dom, exist_ok=True)\n",
    "os.makedirs(path_phillip, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be798ed2-4fa7-4341-862f-1ec0d610459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the paths exist (if necessary)\n",
    "# For Ayana: Logistic Regression & Decision Trees (AttendedBootcamp and BootcampFinish)\n",
    "# Export Ayana's training and test datasets for AttendedBootcamp\n",
    "X_train_attend.to_csv(path_ayana + \"X_train_attend.csv\", index=False)\n",
    "y_train_attend.to_csv(path_ayana + \"y_train_attend.csv\", index=False)\n",
    "X_test_attend.to_csv(path_ayana + \"X_test_attend.csv\", index=False)\n",
    "y_test_attend.to_csv(path_ayana + \"y_test_attend.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b26e0-ce41-448e-8a33-caeeb36d8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Ayana's training and test datasets for BootcampFinish\n",
    "X_train_finish.to_csv(path_ayana + \"X_train_finish.csv\", index=False)\n",
    "y_train_finish.to_csv(path_ayana + \"y_train_finish.csv\", index=False)\n",
    "X_test_finish.to_csv(path_ayana + \"X_test_finish.csv\", index=False)\n",
    "y_test_finish.to_csv(path_ayana + \"y_test_finish.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5110d24-9a6e-4926-a571-eb54a1635556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dc1c4-4baa-4f5b-bd69-482cbf013658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Roberta: Random Forest\n",
    "X_train_attend.to_csv(path_roberta + \"X_train_attend.csv\", index=False)\n",
    "y_train_attend.to_csv(path_roberta + \"y_train_attend.csv\", index=False)\n",
    "X_test_attend.to_csv(path_roberta + \"X_test_attend.csv\", index=False)\n",
    "y_test_attend.to_csv(path_roberta + \"y_test_attend.csv\", index=False)\n",
    "X_train_finish.to_csv(path_roberta + \"X_train_finish.csv\", index=False)\n",
    "y_train_finish.to_csv(path_roberta + \"y_train_finish.csv\", index=False)\n",
    "X_test_finish.to_csv(path_roberta + \"X_test_finish.csv\", index=False)\n",
    "y_test_finish.to_csv(path_roberta + \"y_test_finish.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3828d24-e230-4442-ad53-22128c244e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Dom: SVM & KNN\n",
    "X_train_attend.to_csv(path_dom + \"X_train_attend.csv\", index=False)\n",
    "y_train_attend.to_csv(path_dom + \"y_train_attend.csv\", index=False)\n",
    "X_test_attend.to_csv(path_dom + \"X_test_attend.csv\", index=False)\n",
    "y_test_attend.to_csv(path_dom + \"y_test_attend.csv\", index=False)\n",
    "X_train_finish.to_csv(path_dom + \"X_train_finish.csv\", index=False)\n",
    "y_train_finish.to_csv(path_dom + \"y_train_finish.csv\", index=False)\n",
    "X_test_finish.to_csv(path_dom + \"X_test_finish.csv\", index=False)\n",
    "y_test_finish.to_csv(path_dom + \"y_test_finish.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20933319-45de-432f-9152-e34a56406d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Phillip: XGBoost & LightGBM\n",
    "X_train_attend.to_csv(path_phillip + \"X_train_attend.csv\", index=False)\n",
    "y_train_attend.to_csv(path_phillip + \"y_train_attend.csv\", index=False)\n",
    "X_test_attend.to_csv(path_phillip + \"X_test_attend.csv\", index=False)\n",
    "y_test_attend.to_csv(path_phillip + \"y_test_attend.csv\", index=False)\n",
    "X_train_finish.to_csv(path_phillip + \"X_train_finish.csv\", index=False)\n",
    "y_train_finish.to_csv(path_phillip + \"y_train_finish.csv\", index=False)\n",
    "X_test_finish.to_csv(path_phillip + \"X_test_finish.csv\", index=False)\n",
    "y_test_finish.to_csv(path_phillip + \"y_test_finish.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fc327-6f64-4618-af60-38191ebaa9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can export the clean dataset for each member as well\n",
    "data_cleaned.to_csv(path_ayana + \"clean_dataset.csv\", index=False)\n",
    "data_cleaned.to_csv(path_roberta + \"clean_dataset.csv\", index=False)\n",
    "data_cleaned.to_csv(path_dom + \"clean_dataset.csv\", index=False)\n",
    "data_cleaned.to_csv(path_phillip + \"clean_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d97b75-f364-40c8-9694-bad7f83a3bcb",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7826921-83cd-4213-8fae-c72acbaaf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the percentage of null values in each column\n",
    "X_train.isna().sum()/len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757d3b6-377c-4f5d-9e2d-9592a6d8bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore each column with missing values to determine the best fill strategy\n",
    "# First the job column\n",
    "X_train['<Choose Column>'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47db665-338d-4599-a7f9-01459a566fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
